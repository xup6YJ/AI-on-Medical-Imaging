{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from ast import Sub\n",
    "#from types import NoneType\n",
    "from typing import ForwardRef\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import requests\n",
    "import math\n",
    "from PIL import *\n",
    "from os import listdir\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import isfile, isdir, join, splitext\n",
    "import importlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score, cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold, KFold\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "from torch.optim.swa_utils import *\n",
    "from performance import *\n",
    "\n",
    "\n",
    "def get_Kfold_index(dataset,n_splits=10,random_state=None):\n",
    "    #Statified group k fold based on number of stroke ROIs\n",
    "    skf = StratifiedGroupKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "    labels = np.stack(dataset.label_list,axis=0)\n",
    "    subj_idx = np.arange(len(dataset))\n",
    "    subj_idx = np.tile(subj_idx,(1,*labels.shape[1:])).flatten()\n",
    "    fold_idx = []\n",
    "    fold_iter = skf.split(labels.flatten(), labels.flatten(), subj_idx)\n",
    "    for train_idx, test_idx in fold_iter:\n",
    "        subj_train = np.unique(subj_idx[train_idx])\n",
    "        subj_test = np.unique(subj_idx[test_idx])\n",
    "        fold_idx.append((subj_train,subj_test))\n",
    "    return fold_idx\n",
    "\n",
    "\n",
    "# initialize dataset\n",
    "main_data =  TorchDataset(image_dir = 'D:/File_X/PHD/ASPECT_class/data/wholebrain_CT_pytorch_dataset_ASPECTSCrop_DS',\n",
    "        repeat=1,augment=True,z_crop=False,dtype=torch.float32,pre_align=False,\n",
    "        aug_Rrange=15, aug_Trange=0.02,\n",
    "        hflip=False,vflip=False)    \n",
    "print(\"Main dataset:\\n\", main_data)\n",
    "\n",
    "# initialize Cv index\n",
    "kfold_idx = get_Kfold_index(main_data,n_splits=2,random_state=None)\n",
    "#skf = KFold(n_splits=args.num_folds,shuffle=True,random_state=args.seed)\n",
    "K = 0\n",
    "num_test_aug = 1\n",
    "for train_index, test_index in kfold_idx:\n",
    "    # print('train_index: ', len(train_index),\n",
    "    #       ' test_index: ', len(test_index))\n",
    "    K += 1\n",
    "    if K < 1: # Skip finished fold\n",
    "        continue\n",
    "    print('=====================\\tFold %d\\t======================='%K)\n",
    "\n",
    "    # initialize dataset\n",
    "    train_data = copy.deepcopy(main_data)\n",
    "    valid_data = copy.deepcopy(main_data)\n",
    "    train_data.filter(train_index)\n",
    "    valid_data.filter(test_index)\n",
    "    if  num_test_aug ==1:\n",
    "        valid_data.no_augment()\n",
    "    valid_data.n_augs = num_test_aug\n",
    "    \n",
    "    print(\"Training dataset:\\n\", train_data)\n",
    "    print(\"Validation dataset:\\n\", valid_data)\n",
    "    \n",
    "#Change path\n",
    "os.listdir('data/wholebrain_CT_pytorch_dataset_ASPECTSCrop_DS')\n",
    "mat_list = glob.glob('data/wholebrain_CT_pytorch_dataset_ASPECTSCrop_DS/*.mat')\n",
    "len(mat_list)\n",
    "\n",
    "presentOnDisk_set = [os.path.split(p)[-1] for p in mat_list]\n",
    "\n",
    "for ind in kfold_idx[0][0]:\n",
    "    # print(ind)\n",
    "    original = r'data/wholebrain_CT_pytorch_dataset_ASPECTSCrop_DS/' + presentOnDisk_set[ind]\n",
    "    target = r'data/Train1/' + presentOnDisk_set[ind]\n",
    "    \n",
    "    shutil.copyfile(original, target)\n",
    "\n",
    "for ind in kfold_idx[0][1]:\n",
    "    # print(ind)\n",
    "    original = r'data/wholebrain_CT_pytorch_dataset_ASPECTSCrop_DS/' + presentOnDisk_set[ind]\n",
    "    target = r'data/Train2/' + presentOnDisk_set[ind]\n",
    "    \n",
    "    shutil.copyfile(original, target)\n",
    "\n",
    "\n",
    "#2 fold dataset\n",
    "main_data1 =  TorchDataset(image_dir = 'D:/File_X/PHD/ASPECT_class/data/Train1',\n",
    "        repeat=1,augment=True,z_crop=False,dtype=torch.float32,pre_align=False,\n",
    "        aug_Rrange=15, aug_Trange=0.02,\n",
    "        hflip=False,vflip=False)    \n",
    "print(\"Main dataset 1:\\n\", main_data1)\n",
    "\n",
    "main_data2 =  TorchDataset(image_dir = 'D:/File_X/PHD/ASPECT_class/data/Train2',\n",
    "        repeat=1,augment=True,z_crop=False,dtype=torch.float32,pre_align=False,\n",
    "        aug_Rrange=15, aug_Trange=0.02,\n",
    "        hflip=False,vflip=False)    \n",
    "print(\"Main dataset 2:\\n\", main_data2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset at 0x7f7d10421af0>,\n",
       " <torch.utils.data.dataset.Subset at 0x7f7d0120bc40>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.utils.data as U\n",
    "\n",
    "x = U.random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 52\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "# Load data set\n",
    "train_data_path = '/home/yingchihlin/Documents/Code/AISD/AISD.train'\n",
    "id_list = os.listdir(train_data_path)\n",
    "train_set = sorted(id_list)\n",
    "\n",
    "# Before\n",
    "# print('Train data set:', len(train_set))\n",
    "\n",
    "# Random split\n",
    "train_set_size = int(len(train_set) * 0.85)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "# train_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size])\n",
    "\n",
    "# # After\n",
    "# print('='*30)\n",
    "# print('Train data set:', len(train_set))\n",
    "# print('Valid data set:', len(valid_set))\n",
    "\n",
    "# valid_set\n",
    "print(train_set_size, valid_set_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 52\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "val_set = random.sample(list(train_set), valid_set_size)\n",
    "# print(val_list)\n",
    "\n",
    "# train_data = list(train_set).remove('0538833')\n",
    "[train_set.remove(x) for x in val_set]\n",
    "\n",
    "# subj_names = [os.path.split(image_paths[x])[0][-7:] for x in range(image_num)]\n",
    "\n",
    "print(len(train_set), len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "import glob\n",
    "from logging import root\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "train_image_paths = [glob.glob('/home/yingchihlin/Documents/Code/AISD/AISD.train/{}'.format(uid))[0] for uid in train_set]\n",
    "val_image_paths = [glob.glob('/home/yingchihlin/Documents/Code/AISD/AISD.train/{}'.format(uid))[0] for uid in val_set]\n",
    "\n",
    "root_path = os.getcwd()\n",
    "# print(root_path)\n",
    "\n",
    "train_data_path = os.path.join(root_path, 'train')\n",
    "val_data_path = os.path.join(root_path, 'validation')\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "\n",
    "if not os.path.exists(val_data_path):\n",
    "    os.makedirs(val_data_path)\n",
    "\n",
    "# os.path.split(train_image_paths[0])[1]\n",
    "# train_image_paths[0]\n",
    "\n",
    "# file = train_image_paths[0]\n",
    "# target = os.path.join(train_data_path, os.path.split(file)[1])\n",
    "# print(target)\n",
    "# shutil.copytree(file, target)\n",
    "\n",
    "\n",
    "for file in train_image_paths:\n",
    "\n",
    "    target = os.path.join(train_data_path, os.path.split(file)[1])\n",
    "    shutil.copytree(file, target)\n",
    "\n",
    "for file in val_image_paths:\n",
    "\n",
    "    target = os.path.join(val_data_path, os.path.split(file)[1])\n",
    "    shutil.copytree(file, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6913a215bbcc3bc7ae5e7590d1ced35ed5310329dabd281eb0a320ee4e5a02d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
